For researchers
===========================================

Этот раздел предназначен для исследователей, которые будут разворачивать ARTM Online локально на свойм компьтере.

.. rubric:: Разворачивание ARTM Online

1. Установите python 3 (рекомендуется anaconda).

2. Установите django

.. code-block:: bash 

   pip install django 

   
3. Установите систему управления базами данных. Рекомендуеся PostgreSQL.

4. Создайте базу данных artmonlinedb. Запомните имя пользователя и пароль для доступа к этой базе данных. Имя пользователя по умолчанию в PostgreSQL - postgres.

5. Установите git

6. Склонируйте проект ARTM Online на свой компьютер. Для этого создайте где-нибудь папку artmonline, перейдите в неё и выполните

.. code-block:: bash 

   git clone https://github.com/fedimser/artmonline.git
   
7. Теперь нужно связать проект c базой данных. Сначала откройте файл artmonline/settings.py и найдите там такие строчки

.. code-block:: python 

   DATABASES = {
		'default': {
			'ENGINE': 'django.db.backends.postgresql_psycopg2',
			'NAME': 'artmonlinedb',
			'USER': 'postgres',
			'PASSWORD': '******',
			'HOST': '127.0.0.1',
			'PORT': '5432', 
		}
	}

Внесите изменения, соответствующие вашей СУБД. Если вы используете PostgreSQL с настройками по умолчанию, понадобится изменить только пароль.

8. Теперь нужно, чтобы django создал нужные ему таблицы. Для этого выполните

.. code-block:: bash 

   python manage.py makemigrations
   python manage.py migrate
   
9. Создайте суперпользователя (эти логин и пароль потом нужно будет ввести, когда сервис попросит вас залогиниться)

.. code-block:: bash 

   python manage.py createsuperuser
   
   
10. Запустите сервер

.. code-block:: bash 

   python manage.py runserver
   
11. Зайдите в браузере по адресу http://127.0.0.1:8000

.. rubric:: Начало работы с ARTM Online

Для начала разберёмся с объктной моделью. 

Dataset - это набор документов. Он обязательно содержит словарь (список терминов) и "мешки слов". Опциаонально он может содержать текст документов,
вхождения терминов в этот текст (это будем называть word_index), дополнительную информацию о документах (дата, время, название, 
краткое описание (snippet), ссылку на оригинал в документе).

Термины принадлежат модальностям. Если модальностей нет, считается, что все термины принадлежат модальности @default_class.

Модель (ArtmModel) содержит всю информацию некоторой тематической модели о dataset'е (а именно, матрицы Θ, Φ, Ψ) и производную информацию. 

.. rubric:: Загрузка dataset'а

Итак, для начала работы нужно загрузить dataset. Создайте в папке проекта папку data, в ней папку datasets, в ней папку для каждого dataset'а.
В папке dataset'a создайте папку UCI, в которой нужно будет поместить два файла с описанием словаря и мешков слов. Этого достаточно для того, чтобы
можно было строить модели, но вы не сможете посмотреть текст. Поэтому, ессли у вас есть все документы в текстовом виде, загрузите их в папку
documents, именуя номерами с единицы с расширением .txt. Загрузите "файлы индексов" в папку "word_index". Наконец, создайте файл documents.json 
с мета-данными всех документов. В итоге должна получиться такая файловая структура:
   
.. code-block:: bash 

   artmonline
		data
			datasets
				dataset1
					UCI
						docword.dataset1.txt
						vocab.dataset1.txt
					documents
						1.txt
						2.txt
						.....
					word_index
						1.txt
						2.txt
						.....
					documents.json
							
Теперь подробно разберём все файлы. Файл vocab.dataset1.txt (вместо dataset1 поставьте имя вашего dataset'a, так же должна назыаться папка dataset'a)
должен содержать в себе столько же строк, сколько слов в словаре. На каждой строке должно быть по одному слову. Если у вас есть модельности,
то на каждой строке, после слова, через пробел, напишите название модальностей. Никаких пробелов в терминах быть не доолжно. Если у вас N-граммы,
используйте нижнее подчёркивание.

Файл docword.dataset1.txt содержит мешки слов. В первой строчке записано число документов, 
во второй - число терминов (должно совпадать с числом строк в предыдущем файле), в третьей - число строчек в файле (первые три не считаются).
Во всех остальных строчках записано по три числа - номер документа, номер слова, количество вхождений. Важно, чтобы номера слов соответсвовали 
их положению в словаре (нумерация с единицы).

.. code-block:: bash 

   D
   W
   NNZ
   docID wordID count
   docID wordID count
   ...
   docID wordID count

Примеры можно посмотреть на https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/readme.txt

Важно, чтобы в каждом документе был хотя бы один термин.

Документы в папке documents - просто текст.

Файлы в папке word_index нужны исключительно для того, чтобы подсвечивать слова при визуализации. Не все слова являются терминами, слова в документе
не всегда в начальной форме, иногда термины - это n-граммы. Поэтому необходимо указать, где именно в документе находяся какие термины.
Файлы в папке word_index состоят из нескольких (по числу терминов) строк, в каждой по четыре целых числа

.. code-block:: bash 

   line start_pos length term_id
   
line - это номер строчки (нумерация с единицы), start_pos - начало термина в этой строчке (нкмерация с нуля!), length - длина вхождения,
term_id - номер термина в той же нумерации, что в файле docword.

Наконец, documents.json - это JSON-словарь со всей информацией о документах. Его ключи - это номера документов, 
а значения словари со следующими полями:

.. code-block:: bash 

   title
   snippet
   url
   time
   
Время должно быть JSON-списком из шести целых чисел (год, месяц, день, час, минута, секунда).

Теперь, когда вы поместили все нужные данные, можно их импортировать. Для этого в интерфейсе ARTM Online выберите пунккт Datasets вверху, потом справа
нажмите на Create new dataset... Выберите вкладку Local, в ней из списка Dataset name выберите имя вашего dataset'a (система находит его автоматически,
если вы его поместилии в правильную папку). Теперь правильно установите следующие галочки (это важно!):

* UCI vocab and docword provided - включите обязательно
* documents.json provided - включите, если есть файл documents.json
* text provided - включите, если есть текстовые файлы
* Word index provided - включите, если есть файлы word_index
* Time provided provided - включите, если есть время
 
Нажмите на кнопку Create и подождите, пока dataset импортируется.

Теперь у вас появились папки batches и models в папке с dataset'ом. В папке batches находятся ыходные данные для BigARTM. Используйте только их!

.. rubric:: Создание модели

Для бытрого старта попробуйте встроенные построители моделей. 
Зайдите в Datasets, выберите только что созданный датасет, нажмите справа Create new model...
Выберите Flat или Hierarchical, задайте нужное число тем и нажмите Create.

Рассмотрим другой вариант. Вы будете сами писать 
скрипт на python, строящий модель,
а ARTM Online будет обрабатывать выход вашего скрипта и строить визуализацию.

Сначала зайдите в Datasets, выберите только что чозданный датасет, нажмите справа Create new model... Теперь выберите вкладку Empty и нажмите create.
У вас появится папка в папке models. Там будет образец скрипта, который читает данные из батчей, строит простую модель и пишет матрицы в виде
pandas.dataframe'ов в папку с моделью. Разумеется, Вы можете поместить свой скрипт где угодно, главное читать и писать в правильных местах.

Итак, Ваш скрипт должен сохранять результат работы BigARTM'а. Это матрицы тета и фи. Сохраняйте их с помощью to_pickle() в файлы theta и phi.

Если строите N-уровневую иерархию с помощью hARTM, то надо ещё сохранить файлы psi1, psi2, ..., psi(N-1).

Когда сохранили все матрицы, перейдите на страницу модели (к ней можно попасть или со страницы датасета, ищите справа список моделей, 
или со страницы своего профиля). Там нажмите на Reload Model. Подождите, пока произойдёт импорт. Теперь можете смотреть на физуализацию вашей модели.

Можно работать так. Создайте одну модель, вносите в неё изменения и перезагружайте. 
Можно создаать несколько моделей (например, с разными параметрами). Они все будут доступны одновременнл.

.. rubric:: Визуализации

Вы можете смотреть информацию о следующих объектах: документы, темы, термины, модальности, сама модель.

Кроме того, можно смотреть разные визуализации модели в целом. Список всех доступных визуализаций находится на странице dataset'а.
Если у вас несколько моделей, то с помощью "радио-кнопок" выберите (на странице датасета), какую модель вы хотите использовать для визуализации.